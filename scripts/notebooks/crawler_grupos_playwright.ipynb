{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Crawler Industrial de Grupos WhatsApp M√©dicos\n",
    "\n",
    "Extrai grupos de WhatsApp de todos os principais agregadores de vagas m√©dicas do Brasil.\n",
    "\n",
    "## Sites cobertos:\n",
    "1. **Escala de Plant√£o** - HTML simples (requests)\n",
    "2. **Grupos M√©dicos** - HTML simples (requests)\n",
    "3. **Quero Plant√£o** - JavaScript (Playwright)\n",
    "4. **Search Plant√£o** - JavaScript (Playwright)\n",
    "5. **Plant√µes M√©dicos Brasil** - JavaScript (Playwright)\n",
    "\n",
    "## Requisitos:\n",
    "```bash\n",
    "pip install requests beautifulsoup4 pandas lxml playwright\n",
    "playwright install chromium\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: playwright\n"
     ]
    }
   ],
   "source": [
    "# Instala√ß√£o (rodar apenas uma vez)\n",
    "# !pip install requests beautifulsoup4 pandas lxml playwright -q\n",
    "!playwright install chromium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas carregadas\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "print(\"‚úÖ Bibliotecas carregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fun√ß√µes Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√µes auxiliares carregadas\n"
     ]
    }
   ],
   "source": [
    "def extract_whatsapp_code(url):\n",
    "    \"\"\"Extrai o c√≥digo do convite do link do WhatsApp\"\"\"\n",
    "    match = re.search(r'chat\\.whatsapp\\.com/([A-Za-z0-9]+)', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extract_telegram_code(url):\n",
    "    \"\"\"Extrai o c√≥digo/username do link do Telegram\"\"\"\n",
    "    match = re.search(r't\\.me/(?:joinchat/)?([A-Za-z0-9_-]+)', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def detect_state(text):\n",
    "    \"\"\"Detecta o estado brasileiro no texto\"\"\"\n",
    "    states = {\n",
    "        'AC': ['acre'], 'AL': ['alagoas'], 'AP': ['amap√°', 'amapa'],\n",
    "        'AM': ['amazonas'], 'BA': ['bahia'], 'CE': ['cear√°', 'ceara'],\n",
    "        'DF': ['distrito federal', 'bras√≠lia', 'brasilia', ' df '],\n",
    "        'ES': ['esp√≠rito santo', 'espirito santo'],\n",
    "        'GO': ['goi√°s', 'goias'], 'MA': ['maranh√£o', 'maranhao'],\n",
    "        'MT': ['mato grosso'], 'MS': ['mato grosso do sul'],\n",
    "        'MG': ['minas gerais'], 'PA': ['par√°'],\n",
    "        'PB': ['para√≠ba', 'paraiba'], 'PR': ['paran√°', 'parana'],\n",
    "        'PE': ['pernambuco'], 'PI': ['piau√≠', 'piaui'],\n",
    "        'RJ': ['rio de janeiro'], 'RN': ['rio grande do norte'],\n",
    "        'RS': ['rio grande do sul'], 'RO': ['rond√¥nia', 'rondonia'],\n",
    "        'RR': ['roraima'], 'SC': ['santa catarina'],\n",
    "        'SP': ['s√£o paulo', 'sao paulo', ' sp ', 'interior-sp', 'interior sp'],\n",
    "        'SE': ['sergipe'], 'TO': ['tocantins'],\n",
    "        'NORTE': ['regi√£o norte', 'regiao norte'],\n",
    "        'NORDESTE': ['regi√£o nordeste', 'regiao nordeste', 'nordeste'],\n",
    "        'SUDESTE': ['regi√£o sudeste', 'regiao sudeste', 'sudeste'],\n",
    "        'SUL': ['regi√£o sul', 'regiao sul'],\n",
    "        'CENTRO_OESTE': ['centro-oeste', 'centro oeste']\n",
    "    }\n",
    "    \n",
    "    text_lower = ' ' + text.lower() + ' '\n",
    "    for state_code, patterns in states.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in text_lower:\n",
    "                return state_code\n",
    "    return 'BR'\n",
    "\n",
    "def detect_category(text):\n",
    "    \"\"\"Detecta a categoria/especialidade\"\"\"\n",
    "    categories = {\n",
    "        'plantao_geral': ['plant√£o', 'plantao', 'vagas', 'emprego', 'escala'],\n",
    "        'cardiologia': ['cardiologia', 'cardiologista', 'cardio'],\n",
    "        'pediatria': ['pediatria', 'pediatra', 'pedi√°trico', 'neonat', 'neo '],\n",
    "        'ginecologia': ['ginecologia', 'obstetr√≠cia', 'obstetricia', 'gineco'],\n",
    "        'ortopedia': ['ortopedia', 'ortopedista'],\n",
    "        'neurologia': ['neurologia', 'neurologista', 'neuro'],\n",
    "        'psiquiatria': ['psiquiatria', 'psiquiatra'],\n",
    "        'anestesiologia': ['anestesiologia', 'anestesista', 'anestesia'],\n",
    "        'cirurgia': ['cirurgia', 'cirurgi√£o', 'cirurgiao'],\n",
    "        'dermatologia': ['dermatologia', 'dermatologista'],\n",
    "        'oftalmologia': ['oftalmologia', 'oftalmologista', 'oftalmo'],\n",
    "        'urologia': ['urologia', 'urologista'],\n",
    "        'emergencia': ['emerg√™ncia', 'emergencia', 'urg√™ncia', 'urgencia', 'uti', 'intensiva'],\n",
    "        'oncologia': ['oncologia', 'oncologista'],\n",
    "        'gastroenterologia': ['gastroenterologia', 'gastro'],\n",
    "        'pneumologia': ['pneumologia', 'pneumologista'],\n",
    "        'infectologia': ['infectologia', 'infectologista'],\n",
    "        'nefrologia': ['nefrologia', 'nefrologista'],\n",
    "        'endocrinologia': ['endocrinologia', 'endocrinologista'],\n",
    "        'reumatologia': ['reumatologia', 'reumatologista'],\n",
    "        'geriatria': ['geriatria', 'geriatra'],\n",
    "        'otorrino': ['otorrino', 'otorrinolaringologia'],\n",
    "        'radiologia': ['radiologia', 'radiologista', 'ultrassonografia', 'ecografia'],\n",
    "        'medicina_trabalho': ['trabalho', 'ocupacional'],\n",
    "        'telemedicina': ['telemedicina', 'teleconsulta'],\n",
    "        'medicina_familia': ['fam√≠lia', 'familia', 'comunidade'],\n",
    "        'enfermagem': ['enfermagem', 'enfermeiro', 'enfermeira'],\n",
    "        'odontologia': ['odontologia', 'dentista', 'odonto'],\n",
    "        'fisioterapia': ['fisioterapia', 'fisioterapeuta'],\n",
    "        'nutricao': ['nutri√ß√£o', 'nutricao', 'nutricionista', 'nutrologia'],\n",
    "        'psicologia': ['psicologia', 'psic√≥logo', 'psicologa'],\n",
    "        'fonoaudiologia': ['fonoaudiologia', 'fonoaudi√≥logo'],\n",
    "        'material_estudo': ['material', 'resumo', 'livro', 'artigo', 'pdf'],\n",
    "        'discussao_casos': ['discuss√£o', 'casos cl√≠nicos', 'ecg', 'eco'],\n",
    "        'gestao': ['gest√£o', 'gestao', 'empreendedorismo'],\n",
    "        'concurso_residencia': ['concurso', 'resid√™ncia', 'residencia']\n",
    "    }\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for category, patterns in categories.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in text_lower:\n",
    "                return category\n",
    "    return 'medicina_geral'\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes auxiliares carregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crawler com Requests (sites HTML simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawler requests carregado\n"
     ]
    }
   ],
   "source": [
    "def crawl_with_requests(url, source_name):\n",
    "    \"\"\"Crawler para sites que n√£o usam JavaScript\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao acessar {url}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    groups = []\n",
    "    \n",
    "    # WhatsApp\n",
    "    for link in soup.find_all('a', href=re.compile(r'chat\\.whatsapp\\.com')):\n",
    "        href = link.get('href')\n",
    "        name = link.get_text(strip=True) or 'Sem nome'\n",
    "        \n",
    "        # Tenta pegar contexto do elemento pai\n",
    "        parent = link.find_parent(['div', 'section', 'article', 'li'])\n",
    "        if parent:\n",
    "            title = parent.find(['h2', 'h3', 'h4', 'h5', 'strong', 'b'])\n",
    "            if title:\n",
    "                name = title.get_text(strip=True)\n",
    "        \n",
    "        invite_code = extract_whatsapp_code(href)\n",
    "        if invite_code and len(name) > 2:\n",
    "            groups.append({\n",
    "                'source': source_name,\n",
    "                'platform': 'whatsapp',\n",
    "                'name': name[:100],  # limita tamanho\n",
    "                'url': href,\n",
    "                'invite_code': invite_code,\n",
    "                'state': detect_state(name),\n",
    "                'category': detect_category(name),\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    # Telegram\n",
    "    for link in soup.find_all('a', href=re.compile(r't\\.me')):\n",
    "        href = link.get('href')\n",
    "        name = link.get_text(strip=True) or 'Sem nome'\n",
    "        \n",
    "        invite_code = extract_telegram_code(href)\n",
    "        if invite_code and len(name) > 2:\n",
    "            groups.append({\n",
    "                'source': source_name,\n",
    "                'platform': 'telegram',\n",
    "                'name': name[:100],\n",
    "                'url': href,\n",
    "                'invite_code': invite_code,\n",
    "                'state': detect_state(name),\n",
    "                'category': detect_category(name),\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    return groups\n",
    "\n",
    "print(\"‚úÖ Crawler requests carregado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crawler com Playwright (sites JavaScript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawler Playwright carregado\n"
     ]
    }
   ],
   "source": [
    "def crawl_with_playwright(url, source_name, wait_time=5000, scroll=True):\n",
    "    \"\"\"Crawler para sites que carregam conte√∫do via JavaScript\"\"\"\n",
    "    \n",
    "    groups = []\n",
    "    \n",
    "    with sync_playwright() as p:\n",
    "        # Lan√ßa navegador headless\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        context = browser.new_context(\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        )\n",
    "        page = context.new_page()\n",
    "        \n",
    "        try:\n",
    "            print(f\"   Acessando {url}...\")\n",
    "            page.goto(url, timeout=60000)\n",
    "            \n",
    "            # Espera inicial\n",
    "            page.wait_for_timeout(wait_time)\n",
    "            \n",
    "            # Scroll para carregar conte√∫do lazy-loaded\n",
    "            if scroll:\n",
    "                for _ in range(5):\n",
    "                    page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n",
    "                    page.wait_for_timeout(1000)\n",
    "            \n",
    "            # Tenta clicar em tabs/abas se existirem (comum em sites de grupos)\n",
    "            try:\n",
    "                tabs = page.query_selector_all('[role=\"tab\"], .tab, .nav-link, button[data-toggle]')\n",
    "                for tab in tabs[:10]:  # limita a 10 tabs\n",
    "                    try:\n",
    "                        tab.click()\n",
    "                        page.wait_for_timeout(1500)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Extrai links WhatsApp\n",
    "            whatsapp_links = page.query_selector_all('a[href*=\"chat.whatsapp.com\"]')\n",
    "            print(f\"   Encontrados {len(whatsapp_links)} links WhatsApp\")\n",
    "            \n",
    "            for link in whatsapp_links:\n",
    "                try:\n",
    "                    href = link.get_attribute('href')\n",
    "                    name = link.inner_text().strip() or 'Sem nome'\n",
    "                    \n",
    "                    # Tenta pegar contexto\n",
    "                    try:\n",
    "                        parent = link.evaluate('el => el.closest(\"div, section, article, li\")')\n",
    "                        if parent:\n",
    "                            # Isso √© mais complexo, vamos simplificar\n",
    "                            pass\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    invite_code = extract_whatsapp_code(href)\n",
    "                    if invite_code:\n",
    "                        groups.append({\n",
    "                            'source': source_name,\n",
    "                            'platform': 'whatsapp',\n",
    "                            'name': name[:100] if len(name) > 3 else f'{source_name} - Grupo',\n",
    "                            'url': href,\n",
    "                            'invite_code': invite_code,\n",
    "                            'state': detect_state(name),\n",
    "                            'category': detect_category(name),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Extrai links Telegram\n",
    "            telegram_links = page.query_selector_all('a[href*=\"t.me\"]')\n",
    "            print(f\"   Encontrados {len(telegram_links)} links Telegram\")\n",
    "            \n",
    "            for link in telegram_links:\n",
    "                try:\n",
    "                    href = link.get_attribute('href')\n",
    "                    name = link.inner_text().strip() or 'Sem nome'\n",
    "                    \n",
    "                    invite_code = extract_telegram_code(href)\n",
    "                    if invite_code:\n",
    "                        groups.append({\n",
    "                            'source': source_name,\n",
    "                            'platform': 'telegram',\n",
    "                            'name': name[:100] if len(name) > 3 else f'{source_name} - Grupo',\n",
    "                            'url': href,\n",
    "                            'invite_code': invite_code,\n",
    "                            'state': detect_state(name),\n",
    "                            'category': detect_category(name),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro: {e}\")\n",
    "        finally:\n",
    "            browser.close()\n",
    "    \n",
    "    return groups\n",
    "\n",
    "print(\"‚úÖ Crawler Playwright carregado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crawler espec√≠fico para Quero Plant√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawler Quero Plant√£o carregado\n"
     ]
    }
   ],
   "source": [
    "def crawl_quero_plantao():\n",
    "    \"\"\"Crawler espec√≠fico para queroplantao.com.br/grupos\n",
    "    \n",
    "    Este site tem tabs por estado que precisam ser clicadas.\n",
    "    \"\"\"\n",
    "    \n",
    "    groups = []\n",
    "    url = 'https://queroplantao.com.br/grupos/'\n",
    "    \n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        context = browser.new_context(\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        )\n",
    "        page = context.new_page()\n",
    "        \n",
    "        try:\n",
    "            print(\"   Acessando Quero Plant√£o...\")\n",
    "            page.goto(url, timeout=60000)\n",
    "            page.wait_for_timeout(5000)\n",
    "            \n",
    "            # Fecha popup se existir\n",
    "            try:\n",
    "                page.click('text=Aceito', timeout=3000)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                page.click('[class*=\"close\"]', timeout=2000)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Lista de estados para tentar clicar\n",
    "            estados = [\n",
    "                'Brasil', 'S√£o Paulo', 'Rio de Janeiro', 'Minas Gerais', 'Bahia',\n",
    "                'Paran√°', 'Rio Grande do Sul', 'Santa Catarina', 'Goi√°s',\n",
    "                'Distrito Federal', 'Cear√°', 'Pernambuco', 'Par√°', 'Amazonas',\n",
    "                'Maranh√£o', 'Para√≠ba', 'Rio Grande do Norte', 'Alagoas', 'Sergipe',\n",
    "                'Piau√≠', 'Esp√≠rito Santo', 'Mato Grosso', 'Mato Grosso do Sul',\n",
    "                'Rond√¥nia', 'Acre', 'Amap√°', 'Roraima', 'Tocantins'\n",
    "            ]\n",
    "            \n",
    "            # Scroll inicial\n",
    "            for _ in range(3):\n",
    "                page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n",
    "                page.wait_for_timeout(1000)\n",
    "            \n",
    "            # Tenta clicar em cada aba de estado\n",
    "            for estado in estados:\n",
    "                try:\n",
    "                    # Tenta diferentes seletores\n",
    "                    selectors = [\n",
    "                        f'text=\"{estado}\"',\n",
    "                        f'button:has-text(\"{estado}\")',\n",
    "                        f'a:has-text(\"{estado}\")',\n",
    "                        f'[data-state=\"{estado}\"]',\n",
    "                        f'.tab:has-text(\"{estado}\")',\n",
    "                    ]\n",
    "                    \n",
    "                    for selector in selectors:\n",
    "                        try:\n",
    "                            element = page.query_selector(selector)\n",
    "                            if element:\n",
    "                                element.click()\n",
    "                                page.wait_for_timeout(1500)\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Scroll final\n",
    "            for _ in range(3):\n",
    "                page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n",
    "                page.wait_for_timeout(1000)\n",
    "            \n",
    "            # Extrai todos os links\n",
    "            whatsapp_links = page.query_selector_all('a[href*=\"chat.whatsapp.com\"]')\n",
    "            print(f\"   Encontrados {len(whatsapp_links)} links WhatsApp\")\n",
    "            \n",
    "            for link in whatsapp_links:\n",
    "                try:\n",
    "                    href = link.get_attribute('href')\n",
    "                    name = link.inner_text().strip()\n",
    "                    \n",
    "                    if not name or len(name) < 3:\n",
    "                        # Tenta pegar texto do elemento pai\n",
    "                        try:\n",
    "                            parent_text = link.evaluate('el => el.parentElement?.innerText || \"\"')\n",
    "                            name = parent_text.strip()[:100] if parent_text else 'Quero Plant√£o - Grupo'\n",
    "                        except:\n",
    "                            name = 'Quero Plant√£o - Grupo'\n",
    "                    \n",
    "                    invite_code = extract_whatsapp_code(href)\n",
    "                    if invite_code:\n",
    "                        groups.append({\n",
    "                            'source': 'quero_plantao',\n",
    "                            'platform': 'whatsapp',\n",
    "                            'name': name[:100],\n",
    "                            'url': href,\n",
    "                            'invite_code': invite_code,\n",
    "                            'state': detect_state(name),\n",
    "                            'category': detect_category(name),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Telegram\n",
    "            telegram_links = page.query_selector_all('a[href*=\"t.me\"]')\n",
    "            print(f\"   Encontrados {len(telegram_links)} links Telegram\")\n",
    "            \n",
    "            for link in telegram_links:\n",
    "                try:\n",
    "                    href = link.get_attribute('href')\n",
    "                    name = link.inner_text().strip() or 'Quero Plant√£o - Telegram'\n",
    "                    \n",
    "                    invite_code = extract_telegram_code(href)\n",
    "                    if invite_code:\n",
    "                        groups.append({\n",
    "                            'source': 'quero_plantao',\n",
    "                            'platform': 'telegram',\n",
    "                            'name': name[:100],\n",
    "                            'url': href,\n",
    "                            'invite_code': invite_code,\n",
    "                            'state': detect_state(name),\n",
    "                            'category': detect_category(name),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro: {e}\")\n",
    "        finally:\n",
    "            browser.close()\n",
    "    \n",
    "    return groups\n",
    "\n",
    "print(\"‚úÖ Crawler Quero Plant√£o carregado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Executar Crawlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   INICIANDO CRAWLER INDUSTRIAL\n",
      "======================================================================\n",
      "\n",
      "üîç ESCALA_PLANTAO\n",
      "   URL: https://escaladeplantao.com.br/grupos\n",
      "   ‚úÖ 49 grupos extra√≠dos\n",
      "\n",
      "üîç GRUPOS_MEDICOS\n",
      "   URL: https://gruposmedicos.com.br/\n",
      "   ‚úÖ 322 grupos extra√≠dos\n",
      "\n",
      "üîç QUERO_PLANTAO\n",
      "   URL: https://queroplantao.com.br/grupos/\n",
      "   ‚ùå Erro: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "\n",
      "üîç SEARCH_PLANTAO\n",
      "   URL: https://web.searchplantao.com.br/grupos\n",
      "   ‚ùå Erro: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "\n",
      "üîç PLANTOES_BRASIL\n",
      "   URL: https://plantoesmedicosbrasil.com.br/\n",
      "   ‚ùå Erro: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
      "Please use the Async API instead.\n",
      "\n",
      "======================================================================\n",
      "   TOTAL BRUTO: 371 grupos\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lista de todos os sites para crawlear\n",
    "sites = [\n",
    "    # Sites com HTML simples (requests)\n",
    "    {'url': 'https://escaladeplantao.com.br/grupos', 'name': 'escala_plantao', 'method': 'requests'},\n",
    "    {'url': 'https://gruposmedicos.com.br/', 'name': 'grupos_medicos', 'method': 'requests'},\n",
    "    \n",
    "    # Sites com JavaScript (Playwright)\n",
    "    {'url': 'https://queroplantao.com.br/grupos/', 'name': 'quero_plantao', 'method': 'playwright_custom'},\n",
    "    {'url': 'https://web.searchplantao.com.br/grupos', 'name': 'search_plantao', 'method': 'playwright'},\n",
    "    {'url': 'https://plantoesmedicosbrasil.com.br/', 'name': 'plantoes_brasil', 'method': 'playwright'},\n",
    "]\n",
    "\n",
    "all_groups = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"   INICIANDO CRAWLER INDUSTRIAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for site in sites:\n",
    "    print(f\"\\nüîç {site['name'].upper()}\")\n",
    "    print(f\"   URL: {site['url']}\")\n",
    "    \n",
    "    try:\n",
    "        if site['method'] == 'requests':\n",
    "            groups = crawl_with_requests(site['url'], site['name'])\n",
    "        elif site['method'] == 'playwright_custom' and site['name'] == 'quero_plantao':\n",
    "            groups = crawl_quero_plantao()\n",
    "        else:\n",
    "            groups = crawl_with_playwright(site['url'], site['name'])\n",
    "        \n",
    "        all_groups.extend(groups)\n",
    "        print(f\"   ‚úÖ {len(groups)} grupos extra√≠dos\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro: {e}\")\n",
    "    \n",
    "    # Intervalo entre sites\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"   TOTAL BRUTO: {len(all_groups)} grupos\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Processar e Deduplica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DEDUPLICA√á√ÉO:\n",
      "   Antes: 371 grupos\n",
      "   Depois: 263 grupos\n",
      "   Removidos: 108 duplicatas\n"
     ]
    }
   ],
   "source": [
    "# Cria DataFrame\n",
    "df = pd.DataFrame(all_groups)\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"‚ùå Nenhum grupo encontrado!\")\n",
    "else:\n",
    "    # Remove duplicatas por invite_code\n",
    "    antes = len(df)\n",
    "    df = df.drop_duplicates(subset=['invite_code'], keep='first')\n",
    "    depois = len(df)\n",
    "    \n",
    "    print(f\"\\nüìä DEDUPLICA√á√ÉO:\")\n",
    "    print(f\"   Antes: {antes} grupos\")\n",
    "    print(f\"   Depois: {depois} grupos\")\n",
    "    print(f\"   Removidos: {antes - depois} duplicatas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "   ESTAT√çSTICAS FINAIS\n",
      "======================================================================\n",
      "\n",
      "üìä POR FONTE:\n",
      "source\n",
      "grupos_medicos    214\n",
      "escala_plantao     49\n",
      "\n",
      "üìä POR PLATAFORMA:\n",
      "platform\n",
      "whatsapp    263\n",
      "\n",
      "üìä POR ESTADO (top 15):\n",
      "state\n",
      "BR    176\n",
      "SP      7\n",
      "RJ      6\n",
      "DF      4\n",
      "MT      4\n",
      "PR      3\n",
      "ES      3\n",
      "TO      3\n",
      "MA      3\n",
      "RR      3\n",
      "AP      3\n",
      "AC      3\n",
      "PA      3\n",
      "RS      3\n",
      "MG      3\n",
      "\n",
      "üìä POR CATEGORIA (top 15):\n",
      "category\n",
      "medicina_geral     118\n",
      "plantao_geral       49\n",
      "cirurgia            11\n",
      "emergencia           6\n",
      "cardiologia          6\n",
      "neurologia           5\n",
      "radiologia           5\n",
      "pediatria            5\n",
      "ortopedia            4\n",
      "anestesiologia       4\n",
      "psiquiatria          4\n",
      "infectologia         4\n",
      "geriatria            3\n",
      "dermatologia         3\n",
      "discussao_casos      3\n"
     ]
    }
   ],
   "source": [
    "# Estat√≠sticas\n",
    "if len(df) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"   ESTAT√çSTICAS FINAIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìä POR FONTE:\")\n",
    "    print(df['source'].value_counts().to_string())\n",
    "    \n",
    "    print(f\"\\nüìä POR PLATAFORMA:\")\n",
    "    print(df['platform'].value_counts().to_string())\n",
    "    \n",
    "    print(f\"\\nüìä POR ESTADO (top 15):\")\n",
    "    print(df['state'].value_counts().head(15).to_string())\n",
    "    \n",
    "    print(f\"\\nüìä POR CATEGORIA (top 15):\")\n",
    "    print(df['category'].value_counts().head(15).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filtrar Grupos M√©dicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü©∫ GRUPOS DE M√âDICOS: 261 de 263 total\n",
      "\n",
      "Categorias inclu√≠das:\n",
      "category\n",
      "medicina_geral         118\n",
      "plantao_geral           49\n",
      "cirurgia                11\n",
      "emergencia               6\n",
      "cardiologia              6\n",
      "neurologia               5\n",
      "radiologia               5\n",
      "pediatria                5\n",
      "ortopedia                4\n",
      "anestesiologia           4\n",
      "psiquiatria              4\n",
      "infectologia             4\n",
      "geriatria                3\n",
      "dermatologia             3\n",
      "discussao_casos          3\n",
      "gastroenterologia        3\n",
      "endocrinologia           3\n",
      "urologia                 3\n",
      "reumatologia             3\n",
      "pneumologia              3\n",
      "nefrologia               3\n",
      "material_estudo          2\n",
      "otorrino                 2\n",
      "medicina_familia         2\n",
      "medicina_trabalho        2\n",
      "oncologia                1\n",
      "concurso_residencia      1\n",
      "oftalmologia             1\n",
      "telemedicina             1\n",
      "ginecologia              1\n"
     ]
    }
   ],
   "source": [
    "# Categorias que N√ÉO s√£o de m√©dicos\n",
    "categorias_nao_medicas = [\n",
    "    'enfermagem', 'odontologia', 'fisioterapia', 'psicologia', \n",
    "    'nutricao', 'fonoaudiologia', 'assistencia_social', 'bombeiro',\n",
    "    'socorrista', 'tecnico_enfermagem', 'educacao_fisica', 'farmacia'\n",
    "]\n",
    "\n",
    "# Filtra apenas m√©dicos\n",
    "df_medicos = df[~df['category'].isin(categorias_nao_medicas)].copy()\n",
    "\n",
    "print(f\"\\nü©∫ GRUPOS DE M√âDICOS: {len(df_medicos)} de {len(df)} total\")\n",
    "print(f\"\\nCategorias inclu√≠das:\")\n",
    "print(df_medicos['category'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exportar CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Salvo: grupos_whatsapp_saude_COMPLETO.csv (263 grupos)\n",
      "‚úÖ Salvo: grupos_whatsapp_medicos_COMPLETO.csv (261 grupos)\n",
      "‚úÖ Salvo: grupos_whatsapp_medicos_POR_ESTADO.csv (80 grupos)\n",
      "‚úÖ Salvo: grupos_whatsapp_EVOLUTION_API.csv (261 grupos)\n"
     ]
    }
   ],
   "source": [
    "# Salva todos os grupos\n",
    "df.to_csv('grupos_whatsapp_saude_COMPLETO.csv', index=False)\n",
    "print(f\"‚úÖ Salvo: grupos_whatsapp_saude_COMPLETO.csv ({len(df)} grupos)\")\n",
    "\n",
    "# Salva apenas m√©dicos\n",
    "df_medicos.to_csv('grupos_whatsapp_medicos_COMPLETO.csv', index=False)\n",
    "print(f\"‚úÖ Salvo: grupos_whatsapp_medicos_COMPLETO.csv ({len(df_medicos)} grupos)\")\n",
    "\n",
    "# Salva por estado (√∫til para opera√ß√£o)\n",
    "estados_brasil = ['AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', \n",
    "                  'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN',\n",
    "                  'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO']\n",
    "df_por_estado = df_medicos[df_medicos['state'].isin(estados_brasil)]\n",
    "df_por_estado.to_csv('grupos_whatsapp_medicos_POR_ESTADO.csv', index=False)\n",
    "print(f\"‚úÖ Salvo: grupos_whatsapp_medicos_POR_ESTADO.csv ({len(df_por_estado)} grupos)\")\n",
    "\n",
    "# Salva apenas WhatsApp (para Evolution API)\n",
    "df_whatsapp = df_medicos[df_medicos['platform'] == 'whatsapp']\n",
    "df_whatsapp.to_csv('grupos_whatsapp_EVOLUTION_API.csv', index=False)\n",
    "print(f\"‚úÖ Salvo: grupos_whatsapp_EVOLUTION_API.csv ({len(df_whatsapp)} grupos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizar Amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "   AMOSTRA DE GRUPOS POR ESTADO\n",
      "======================================================================\n",
      "\n",
      "üìç SP: 7 grupos\n",
      "   ‚Ä¢ Escala de Plant√£o SP  01\n",
      "     C√≥digo: CIf7BTOKuqp6EUgeE4Pcjp\n",
      "   ‚Ä¢ Escala de Plant√£o SP 02\n",
      "     C√≥digo: J0Hca4lB78gJzJyocrWAUj\n",
      "   ‚Ä¢ Escala de  Plant√£o  Interior-SP 01\n",
      "     C√≥digo: LZODYjNSqwrHTtbDvkb8tg\n",
      "\n",
      "üìç RJ: 6 grupos\n",
      "   ‚Ä¢ Escala de Plant√£o Rio de Janeiro 01\n",
      "     C√≥digo: FPKxAWoN4rMF9GthPmIwHP\n",
      "   ‚Ä¢ Escala de Plant√£o Rio de Janeiro 02\n",
      "     C√≥digo: LqCB1ySd5kxBUqAWcBPv9u\n",
      "   ‚Ä¢ Escala de Plant√£o Rio de Janeiro 03\n",
      "     C√≥digo: IBaiYgVtn6nJR8xRFYUwst\n",
      "\n",
      "üìç MG: 3 grupos\n",
      "   ‚Ä¢ Escala de Plant√£o Minas Gerais\n",
      "     C√≥digo: IydHJ0oQ8xL0MXss1kXCrK\n",
      "   ‚Ä¢ MINAS GERAIS\n",
      "     C√≥digo: BCMu72FYzgl4nCQw807Z0j\n",
      "   ‚Ä¢ Minas Gerais\n",
      "     C√≥digo: HUjNciOWxxDLwfZmD74nFa\n",
      "\n",
      "üìç BA: 3 grupos\n",
      "   ‚Ä¢ Escala de Plant√£o Bahia 01\n",
      "     C√≥digo: LlYuVS8yBXzCOoHWw29q7K\n",
      "   ‚Ä¢ Escala de Plant√£o Bahia 02\n",
      "     C√≥digo: Iypus185ZIwKyVyWoewWfc\n",
      "   ‚Ä¢ BAHIA\n",
      "     C√≥digo: FRhsQyjcTe87e5lDYF84Ml\n",
      "\n",
      "üìç PR: 3 grupos\n",
      "   ‚Ä¢ Escala de Plant√£o Paran√°\n",
      "     C√≥digo: FAgwnBaA4cD29qNt3EuGvS\n",
      "   ‚Ä¢ PARAN√Å\n",
      "     C√≥digo: Dw1TSVcSNBbAEWiS0u0Kd8\n",
      "   ‚Ä¢ Paran√°\n",
      "     C√≥digo: Ly3OTWHFgsk6FdjJ6ulKpk\n",
      "\n",
      "üìç RS: 3 grupos\n",
      "   ‚Ä¢ Escala de Plant√£o Rio Grande do Sul\n",
      "     C√≥digo: IA5DfhBJC0CBrVjRHkCa1r\n",
      "   ‚Ä¢ RIO GRANDE DO SUL\n",
      "     C√≥digo: JK8vXsQbNEP9TbN3iCESsD\n",
      "   ‚Ä¢ Rio Grande do Sul\n",
      "     C√≥digo: FCWi3SOFQV0IPRT2aKoJcY\n",
      "\n",
      "üìç DF: 4 grupos\n",
      "   ‚Ä¢ Escala de Plant√£o DF 01\n",
      "     C√≥digo: KaxcpyFeEWG9L3HEw5Dwhd\n",
      "   ‚Ä¢ Escala de Plant√£o DF 02\n",
      "     C√≥digo: JpLHFKW5mig9DM9Thm6S8Z\n",
      "   ‚Ä¢ DISTRITO FEDERAL\n",
      "     C√≥digo: HFLcaWT3a356xDhJjTLqG3\n"
     ]
    }
   ],
   "source": [
    "# Mostra amostra por estado\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   AMOSTRA DE GRUPOS POR ESTADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for state in ['SP', 'RJ', 'MG', 'BA', 'PR', 'RS', 'DF']:\n",
    "    state_groups = df_medicos[df_medicos['state'] == state]\n",
    "    if len(state_groups) > 0:\n",
    "        print(f\"\\nüìç {state}: {len(state_groups)} grupos\")\n",
    "        for _, row in state_groups.head(3).iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['name'][:50]}\")\n",
    "            print(f\"     C√≥digo: {row['invite_code']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "   AMOSTRA DE GRUPOS POR ESPECIALIDADE\n",
      "======================================================================\n",
      "\n",
      "üè• CARDIOLOGIA: 6 grupos\n",
      "   ‚Ä¢ Cardiologia Pedi√°trica\n",
      "   ‚Ä¢ Cirurgia Cardiovascular\n",
      "\n",
      "üè• PEDIATRIA: 5 grupos\n",
      "   ‚Ä¢ CTI Pedi√°trico\n",
      "   ‚Ä¢ Neonatologia\n",
      "\n",
      "üè• EMERGENCIA: 6 grupos\n",
      "   ‚Ä¢ Urg√™ncia e Emerg√™ncia üö® 3\n",
      "   ‚Ä¢ UTI\n",
      "\n",
      "üè• ORTOPEDIA: 4 grupos\n",
      "   ‚Ä¢ Ortopedista\n",
      "   ‚Ä¢ üü¢ORTOPEDIA ü¶¥\n",
      "\n",
      "üè• ANESTESIOLOGIA: 4 grupos\n",
      "   ‚Ä¢ Anestesiologia\n",
      "   ‚Ä¢ ANESTESIOLOGIA\n"
     ]
    }
   ],
   "source": [
    "# Mostra amostra por especialidade\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   AMOSTRA DE GRUPOS POR ESPECIALIDADE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for cat in ['cardiologia', 'pediatria', 'emergencia', 'ortopedia', 'anestesiologia']:\n",
    "    cat_groups = df_medicos[df_medicos['category'] == cat]\n",
    "    if len(cat_groups) > 0:\n",
    "        print(f\"\\nüè• {cat.upper()}: {len(cat_groups)} grupos\")\n",
    "        for _, row in cat_groups.head(2).iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['name'][:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pr√≥ximos Passos\n",
    "\n",
    "### Para entrar nos grupos via Evolution API:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import time\n",
    "\n",
    "EVOLUTION_URL = \"https://seu-evolution.com\"\n",
    "API_KEY = \"sua-api-key\"\n",
    "\n",
    "def join_group(instance_name, invite_code):\n",
    "    \"\"\"Entra em um grupo de WhatsApp\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/group/acceptInvite/{instance_name}\"\n",
    "    \n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json={\"inviteCode\": invite_code},\n",
    "        headers={\"apikey\": API_KEY}\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# L√™ o CSV\n",
    "df = pd.read_csv('grupos_whatsapp_EVOLUTION_API.csv')\n",
    "\n",
    "# Entra em cada grupo (com rate limiting)\n",
    "for i, row in df.iterrows():\n",
    "    result = join_group('julia-01', row['invite_code'])\n",
    "    print(f\"Grupo {row['name']}: {result}\")\n",
    "    \n",
    "    # M√°ximo 5 grupos por dia por n√∫mero\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(\"Limite di√°rio atingido, trocar n√∫mero ou aguardar\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(60)  # 1 minuto entre cada entrada\n",
    "```\n",
    "\n",
    "### Regras de seguran√ßa:\n",
    "- M√°ximo **5 grupos por dia** por n√∫mero\n",
    "- Esperar **2-3 dias** antes de come√ßar a interagir\n",
    "- Rotacionar entre m√∫ltiplos n√∫meros\n",
    "- Monitorar banimentos e substituir n√∫meros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "   RESUMO FINAL\n",
      "======================================================================\n",
      "\n",
      "üìä GRUPOS EXTRA√çDOS:\n",
      "   Total (todas √°reas):     263 grupos\n",
      "   Apenas m√©dicos:          261 grupos\n",
      "   Por estado:              80 grupos\n",
      "   WhatsApp (Evolution):    261 grupos\n",
      "\n",
      "üìÅ ARQUIVOS GERADOS:\n",
      "   ‚Ä¢ grupos_whatsapp_saude_COMPLETO.csv\n",
      "   ‚Ä¢ grupos_whatsapp_medicos_COMPLETO.csv\n",
      "   ‚Ä¢ grupos_whatsapp_medicos_POR_ESTADO.csv\n",
      "   ‚Ä¢ grupos_whatsapp_EVOLUTION_API.csv\n",
      "\n",
      "üöÄ PR√ìXIMO PASSO:\n",
      "   Use o arquivo grupos_whatsapp_EVOLUTION_API.csv\n",
      "   para integrar com a Evolution API e fazer a\n",
      "   J√∫lia entrar nos grupos automaticamente.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resumo final\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   RESUMO FINAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "üìä GRUPOS EXTRA√çDOS:\n",
    "   Total (todas √°reas):     {len(df)} grupos\n",
    "   Apenas m√©dicos:          {len(df_medicos)} grupos\n",
    "   Por estado:              {len(df_por_estado)} grupos\n",
    "   WhatsApp (Evolution):    {len(df_whatsapp)} grupos\n",
    "\n",
    "üìÅ ARQUIVOS GERADOS:\n",
    "   ‚Ä¢ grupos_whatsapp_saude_COMPLETO.csv\n",
    "   ‚Ä¢ grupos_whatsapp_medicos_COMPLETO.csv\n",
    "   ‚Ä¢ grupos_whatsapp_medicos_POR_ESTADO.csv\n",
    "   ‚Ä¢ grupos_whatsapp_EVOLUTION_API.csv\n",
    "\n",
    "üöÄ PR√ìXIMO PASSO:\n",
    "   Use o arquivo grupos_whatsapp_EVOLUTION_API.csv\n",
    "   para integrar com a Evolution API e fazer a\n",
    "   J√∫lia entrar nos grupos automaticamente.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
