{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Crawler Industrial de Grupos WhatsApp M√©dicos\n",
    "\n",
    "Extrai grupos de WhatsApp de todos os principais agregadores de vagas m√©dicas do Brasil.\n",
    "\n",
    "## Sites cobertos:\n",
    "1. **Escala de Plant√£o** - HTML simples (requests)\n",
    "2. **Grupos M√©dicos** - HTML simples (requests)\n",
    "3. **Quero Plant√£o** - JavaScript (Playwright Async)\n",
    "4. **Search Plant√£o** - JavaScript (Playwright Async)\n",
    "5. **Plant√µes M√©dicos Brasil** - JavaScript (Playwright Async)\n",
    "\n",
    "## Requisitos:\n",
    "```bash\n",
    "pip install requests beautifulsoup4 pandas lxml playwright nest-asyncio\n",
    "playwright install chromium\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o (rodar apenas uma vez)\n",
    "!pip install requests beautifulsoup4 pandas lxml playwright nest-asyncio -q\n",
    "!playwright install chromium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas carregadas\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from playwright.async_api import async_playwright\n",
    "import nest_asyncio\n",
    "\n",
    "# Permite rodar asyncio dentro do Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ Bibliotecas carregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fun√ß√µes Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√µes auxiliares carregadas\n"
     ]
    }
   ],
   "source": [
    "def extract_whatsapp_code(url):\n",
    "    \"\"\"Extrai o c√≥digo do convite do link do WhatsApp\"\"\"\n",
    "    match = re.search(r'chat\\.whatsapp\\.com/([A-Za-z0-9]+)', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def extract_telegram_code(url):\n",
    "    \"\"\"Extrai o c√≥digo/username do link do Telegram\"\"\"\n",
    "    match = re.search(r't\\.me/(?:joinchat/)?([A-Za-z0-9_-]+)', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def detect_state(text):\n",
    "    \"\"\"Detecta o estado brasileiro no texto\"\"\"\n",
    "    states = {\n",
    "        'AC': ['acre'], 'AL': ['alagoas'], 'AP': ['amap√°', 'amapa'],\n",
    "        'AM': ['amazonas'], 'BA': ['bahia'], 'CE': ['cear√°', 'ceara'],\n",
    "        'DF': ['distrito federal', 'bras√≠lia', 'brasilia', ' df '],\n",
    "        'ES': ['esp√≠rito santo', 'espirito santo'],\n",
    "        'GO': ['goi√°s', 'goias'], 'MA': ['maranh√£o', 'maranhao'],\n",
    "        'MT': ['mato grosso'], 'MS': ['mato grosso do sul'],\n",
    "        'MG': ['minas gerais'], 'PA': ['par√°'],\n",
    "        'PB': ['para√≠ba', 'paraiba'], 'PR': ['paran√°', 'parana'],\n",
    "        'PE': ['pernambuco'], 'PI': ['piau√≠', 'piaui'],\n",
    "        'RJ': ['rio de janeiro'], 'RN': ['rio grande do norte'],\n",
    "        'RS': ['rio grande do sul'], 'RO': ['rond√¥nia', 'rondonia'],\n",
    "        'RR': ['roraima'], 'SC': ['santa catarina'],\n",
    "        'SP': ['s√£o paulo', 'sao paulo', ' sp ', 'interior-sp', 'interior sp'],\n",
    "        'SE': ['sergipe'], 'TO': ['tocantins'],\n",
    "        'NORTE': ['regi√£o norte', 'regiao norte'],\n",
    "        'NORDESTE': ['regi√£o nordeste', 'regiao nordeste', 'nordeste'],\n",
    "        'SUDESTE': ['regi√£o sudeste', 'regiao sudeste', 'sudeste'],\n",
    "        'SUL': ['regi√£o sul', 'regiao sul'],\n",
    "        'CENTRO_OESTE': ['centro-oeste', 'centro oeste']\n",
    "    }\n",
    "    \n",
    "    text_lower = ' ' + text.lower() + ' '\n",
    "    for state_code, patterns in states.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in text_lower:\n",
    "                return state_code\n",
    "    return 'BR'\n",
    "\n",
    "def detect_category(text):\n",
    "    \"\"\"Detecta a categoria/especialidade\"\"\"\n",
    "    categories = {\n",
    "        'plantao_geral': ['plant√£o', 'plantao', 'vagas', 'emprego', 'escala'],\n",
    "        'cardiologia': ['cardiologia', 'cardiologista', 'cardio'],\n",
    "        'pediatria': ['pediatria', 'pediatra', 'pedi√°trico', 'neonat', 'neo '],\n",
    "        'ginecologia': ['ginecologia', 'obstetr√≠cia', 'obstetricia', 'gineco'],\n",
    "        'ortopedia': ['ortopedia', 'ortopedista'],\n",
    "        'neurologia': ['neurologia', 'neurologista', 'neuro'],\n",
    "        'psiquiatria': ['psiquiatria', 'psiquiatra'],\n",
    "        'anestesiologia': ['anestesiologia', 'anestesista', 'anestesia'],\n",
    "        'cirurgia': ['cirurgia', 'cirurgi√£o', 'cirurgiao'],\n",
    "        'dermatologia': ['dermatologia', 'dermatologista'],\n",
    "        'oftalmologia': ['oftalmologia', 'oftalmologista', 'oftalmo'],\n",
    "        'urologia': ['urologia', 'urologista'],\n",
    "        'emergencia': ['emerg√™ncia', 'emergencia', 'urg√™ncia', 'urgencia', 'uti', 'intensiva'],\n",
    "        'oncologia': ['oncologia', 'oncologista'],\n",
    "        'gastroenterologia': ['gastroenterologia', 'gastro'],\n",
    "        'pneumologia': ['pneumologia', 'pneumologista'],\n",
    "        'infectologia': ['infectologia', 'infectologista'],\n",
    "        'nefrologia': ['nefrologia', 'nefrologista'],\n",
    "        'endocrinologia': ['endocrinologia', 'endocrinologista'],\n",
    "        'reumatologia': ['reumatologia', 'reumatologista'],\n",
    "        'geriatria': ['geriatria', 'geriatra'],\n",
    "        'otorrino': ['otorrino', 'otorrinolaringologia'],\n",
    "        'radiologia': ['radiologia', 'radiologista', 'ultrassonografia', 'ecografia'],\n",
    "        'medicina_trabalho': ['trabalho', 'ocupacional'],\n",
    "        'telemedicina': ['telemedicina', 'teleconsulta'],\n",
    "        'medicina_familia': ['fam√≠lia', 'familia', 'comunidade'],\n",
    "        'enfermagem': ['enfermagem', 'enfermeiro', 'enfermeira'],\n",
    "        'odontologia': ['odontologia', 'dentista', 'odonto'],\n",
    "        'fisioterapia': ['fisioterapia', 'fisioterapeuta'],\n",
    "        'nutricao': ['nutri√ß√£o', 'nutricao', 'nutricionista', 'nutrologia'],\n",
    "        'psicologia': ['psicologia', 'psic√≥logo', 'psicologa'],\n",
    "        'fonoaudiologia': ['fonoaudiologia', 'fonoaudi√≥logo'],\n",
    "        'material_estudo': ['material', 'resumo', 'livro', 'artigo', 'pdf'],\n",
    "        'discussao_casos': ['discuss√£o', 'casos cl√≠nicos', 'ecg', 'eco'],\n",
    "        'gestao': ['gest√£o', 'gestao', 'empreendedorismo'],\n",
    "        'concurso_residencia': ['concurso', 'resid√™ncia', 'residencia']\n",
    "    }\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for category, patterns in categories.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in text_lower:\n",
    "                return category\n",
    "    return 'medicina_geral'\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes auxiliares carregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crawler com Requests (sites HTML simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawler requests carregado\n"
     ]
    }
   ],
   "source": [
    "def crawl_with_requests(url, source_name):\n",
    "    \"\"\"Crawler para sites que n√£o usam JavaScript\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro ao acessar {url}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    groups = []\n",
    "    \n",
    "    # WhatsApp\n",
    "    for link in soup.find_all('a', href=re.compile(r'chat\\.whatsapp\\.com')):\n",
    "        href = link.get('href')\n",
    "        name = link.get_text(strip=True) or 'Sem nome'\n",
    "        \n",
    "        # Tenta pegar contexto do elemento pai\n",
    "        parent = link.find_parent(['div', 'section', 'article', 'li'])\n",
    "        if parent:\n",
    "            title = parent.find(['h2', 'h3', 'h4', 'h5', 'strong', 'b'])\n",
    "            if title:\n",
    "                name = title.get_text(strip=True)\n",
    "        \n",
    "        invite_code = extract_whatsapp_code(href)\n",
    "        if invite_code and len(name) > 2:\n",
    "            groups.append({\n",
    "                'source': source_name,\n",
    "                'platform': 'whatsapp',\n",
    "                'name': name[:100],\n",
    "                'url': href,\n",
    "                'invite_code': invite_code,\n",
    "                'state': detect_state(name),\n",
    "                'category': detect_category(name),\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    # Telegram\n",
    "    for link in soup.find_all('a', href=re.compile(r't\\.me')):\n",
    "        href = link.get('href')\n",
    "        name = link.get_text(strip=True) or 'Sem nome'\n",
    "        \n",
    "        invite_code = extract_telegram_code(href)\n",
    "        if invite_code and len(name) > 2:\n",
    "            groups.append({\n",
    "                'source': source_name,\n",
    "                'platform': 'telegram',\n",
    "                'name': name[:100],\n",
    "                'url': href,\n",
    "                'invite_code': invite_code,\n",
    "                'state': detect_state(name),\n",
    "                'category': detect_category(name),\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    return groups\n",
    "\n",
    "print(\"‚úÖ Crawler requests carregado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crawler com Playwright ASYNC (sites JavaScript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawler Playwright Async carregado\n"
     ]
    }
   ],
   "source": [
    "async def crawl_with_playwright_async(url, source_name, wait_time=5000, scroll=True):\n",
    "    \"\"\"Crawler ASYNC para sites que carregam conte√∫do via JavaScript\"\"\"\n",
    "    \n",
    "    groups = []\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        \n",
    "        try:\n",
    "            print(f\"   Acessando {url}...\")\n",
    "            await page.goto(url, timeout=60000)\n",
    "            \n",
    "            # Espera inicial\n",
    "            await page.wait_for_timeout(wait_time)\n",
    "            \n",
    "            # Scroll para carregar conte√∫do lazy-loaded\n",
    "            if scroll:\n",
    "                for _ in range(5):\n",
    "                    await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n",
    "                    await page.wait_for_timeout(1000)\n",
    "            \n",
    "            # Extrai links WhatsApp\n",
    "            whatsapp_links = await page.query_selector_all('a[href*=\"chat.whatsapp.com\"]')\n",
    "            print(f\"   Encontrados {len(whatsapp_links)} links WhatsApp\")\n",
    "            \n",
    "            for link in whatsapp_links:\n",
    "                try:\n",
    "                    href = await link.get_attribute('href')\n",
    "                    name = await link.inner_text()\n",
    "                    name = name.strip() if name else 'Sem nome'\n",
    "                    \n",
    "                    invite_code = extract_whatsapp_code(href)\n",
    "                    if invite_code:\n",
    "                        groups.append({\n",
    "                            'source': source_name,\n",
    "                            'platform': 'whatsapp',\n",
    "                            'name': name[:100] if len(name) > 3 else f'{source_name} - Grupo',\n",
    "                            'url': href,\n",
    "                            'invite_code': invite_code,\n",
    "                            'state': detect_state(name),\n",
    "                            'category': detect_category(name),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Extrai links Telegram\n",
    "            telegram_links = await page.query_selector_all('a[href*=\"t.me\"]')\n",
    "            print(f\"   Encontrados {len(telegram_links)} links Telegram\")\n",
    "            \n",
    "            for link in telegram_links:\n",
    "                try:\n",
    "                    href = await link.get_attribute('href')\n",
    "                    name = await link.inner_text()\n",
    "                    name = name.strip() if name else 'Sem nome'\n",
    "                    \n",
    "                    invite_code = extract_telegram_code(href)\n",
    "                    if invite_code:\n",
    "                        groups.append({\n",
    "                            'source': source_name,\n",
    "                            'platform': 'telegram',\n",
    "                            'name': name[:100] if len(name) > 3 else f'{source_name} - Grupo',\n",
    "                            'url': href,\n",
    "                            'invite_code': invite_code,\n",
    "                            'state': detect_state(name),\n",
    "                            'category': detect_category(name),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro: {e}\")\n",
    "        finally:\n",
    "            await browser.close()\n",
    "    \n",
    "    return groups\n",
    "\n",
    "print(\"‚úÖ Crawler Playwright Async carregado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crawler espec√≠fico para Quero Plant√£o (ASYNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Crawler Quero Plant√£o Async carregado\n"
     ]
    }
   ],
   "source": [
    "async def crawl_quero_plantao_async():\n",
    "    \"\"\"Crawler ASYNC espec√≠fico para queroplantao.com.br/grupos\n",
    "    \n",
    "    Este site tem tabs por estado que precisam ser clicadas.\n",
    "    \"\"\"\n",
    "    \n",
    "    groups = []\n",
    "    url = 'https://queroplantao.com.br/grupos/'\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        \n",
    "        try:\n",
    "            print(\"   Acessando Quero Plant√£o...\")\n",
    "            await page.goto(url, timeout=60000)\n",
    "            await page.wait_for_timeout(5000)\n",
    "            \n",
    "            # Fecha popup se existir\n",
    "            try:\n",
    "                await page.click('text=Aceito', timeout=3000)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                close_buttons = await page.query_selector_all('[class*=\"close\"], .close, button[aria-label=\"Close\"]')\n",
    "                for btn in close_buttons:\n",
    "                    try:\n",
    "                        await btn.click(timeout=1000)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Lista de estados para tentar clicar\n",
    "            estados = [\n",
    "                'Brasil', 'S√£o Paulo', 'Rio de Janeiro', 'Minas Gerais', 'Bahia',\n",
    "                'Paran√°', 'Rio Grande do Sul', 'Santa Catarina', 'Goi√°s',\n",
    "                'Distrito Federal', 'Cear√°', 'Pernambuco', 'Par√°', 'Amazonas',\n",
    "                'Maranh√£o', 'Para√≠ba', 'Rio Grande do Norte', 'Alagoas', 'Sergipe',\n",
    "                'Piau√≠', 'Esp√≠rito Santo', 'Mato Grosso', 'Mato Grosso do Sul',\n",
    "                'Rond√¥nia', 'Acre', 'Amap√°', 'Roraima', 'Tocantins'\n",
    "            ]\n",
    "            \n",
    "            # Scroll inicial\n",
    "            for _ in range(3):\n",
    "                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n",
    "                await page.wait_for_timeout(1000)\n",
    "            \n",
    "            # Tenta clicar em cada aba de estado\n",
    "            for estado in estados:\n",
    "                try:\n",
    "                    # Tenta clicar no texto do estado\n",
    "                    element = await page.query_selector(f'text=\"{estado}\"')\n",
    "                    if element:\n",
    "                        await element.click(timeout=2000)\n",
    "                        await page.wait_for_timeout(1500)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Scroll final\n",
    "            for _ in range(3):\n",
    "                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n",
    "                await page.wait_for_timeout(1000)\n",
    "            \n",
    "            # Extrai todos os links WhatsApp\n",
    "            whatsapp_links = await page.query_selector_all('a[href*=\"chat.whatsapp.com\"]')\n",
    "            print(f\"   Encontrados {len(whatsapp_links)} links WhatsApp\")\n",
    "            \n",
    "            for link in whatsapp_links:\n",
    "                try:\n",
    "                    href = await link.get_attribute('href')\n",
    "                    name = await link.inner_text()\n",
    "                    name = name.strip() if name else ''\n",
    "                    \n",
    "                    if not name or len(name) < 3:\n",
    "                        # Tenta pegar texto do elemento pai\n",
    "                        try:\n",
    "                            parent_text = await link.evaluate('el => el.parentElement?.innerText || \"\"')\n",
    "                            name = parent_text.strip()[:100] if parent_text else 'Quero Plant√£o - Grupo'\n",
    "                        except:\n",
    "                            name = 'Quero Plant√£o - Grupo'\n",
    "                    \n",
    "                    invite_code = extract_whatsapp_code(href)\n",
    "                    if invite_code:\n",
    "                        groups.append({\n",
    "                            'source': 'quero_plantao',\n",
    "                            'platform': 'whatsapp',\n",
    "                            'name': name[:100],\n",
    "                            'url': href,\n",
    "                            'invite_code': invite_code,\n",
    "                            'state': detect_state(name),\n",
    "                            'category': detect_category(name),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Telegram\n",
    "            telegram_links = await page.query_selector_all('a[href*=\"t.me\"]')\n",
    "            print(f\"   Encontrados {len(telegram_links)} links Telegram\")\n",
    "            \n",
    "            for link in telegram_links:\n",
    "                try:\n",
    "                    href = await link.get_attribute('href')\n",
    "                    name = await link.inner_text()\n",
    "                    name = name.strip() if name else 'Quero Plant√£o - Telegram'\n",
    "                    \n",
    "                    invite_code = extract_telegram_code(href)\n",
    "                    if invite_code:\n",
    "                        groups.append({\n",
    "                            'source': 'quero_plantao',\n",
    "                            'platform': 'telegram',\n",
    "                            'name': name[:100],\n",
    "                            'url': href,\n",
    "                            'invite_code': invite_code,\n",
    "                            'state': detect_state(name),\n",
    "                            'category': detect_category(name),\n",
    "                            'scraped_at': datetime.now().isoformat()\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro: {e}\")\n",
    "        finally:\n",
    "            await browser.close()\n",
    "    \n",
    "    return groups\n",
    "\n",
    "print(\"‚úÖ Crawler Quero Plant√£o Async carregado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Executar Crawlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   INICIANDO CRAWLER INDUSTRIAL\n",
      "======================================================================\n",
      "\n",
      "üîç ESCALA_PLANTAO\n",
      "   URL: https://escaladeplantao.com.br/grupos\n",
      "   ‚úÖ 49 grupos extra√≠dos\n",
      "\n",
      "üîç GRUPOS_MEDICOS\n",
      "   URL: https://gruposmedicos.com.br/\n",
      "   ‚úÖ 322 grupos extra√≠dos\n",
      "\n",
      "üîç QUERO_PLANTAO\n",
      "   URL: https://queroplantao.com.br/grupos/\n",
      "   Acessando Quero Plant√£o...\n",
      "   Encontrados 0 links WhatsApp\n",
      "   Encontrados 0 links Telegram\n",
      "   ‚úÖ 0 grupos extra√≠dos\n",
      "\n",
      "üîç SEARCH_PLANTAO\n",
      "   URL: https://web.searchplantao.com.br/grupos\n",
      "   Acessando https://web.searchplantao.com.br/grupos...\n",
      "   Encontrados 0 links WhatsApp\n",
      "   Encontrados 0 links Telegram\n",
      "   ‚úÖ 0 grupos extra√≠dos\n",
      "\n",
      "üîç PLANTOES_BRASIL\n",
      "   URL: https://plantoesmedicosbrasil.com.br/\n",
      "   Acessando https://plantoesmedicosbrasil.com.br/...\n",
      "   Encontrados 0 links WhatsApp\n",
      "   Encontrados 1 links Telegram\n",
      "   ‚úÖ 1 grupos extra√≠dos\n",
      "\n",
      "======================================================================\n",
      "   TOTAL BRUTO: 372 grupos\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "async def run_all_crawlers():\n",
    "    \"\"\"Executa todos os crawlers e retorna os grupos\"\"\"\n",
    "    \n",
    "    all_groups = []\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"   INICIANDO CRAWLER INDUSTRIAL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ============================================\n",
    "    # SITES COM HTML SIMPLES (requests)\n",
    "    # ============================================\n",
    "    \n",
    "    print(f\"\\nüîç ESCALA_PLANTAO\")\n",
    "    print(f\"   URL: https://escaladeplantao.com.br/grupos\")\n",
    "    groups = crawl_with_requests('https://escaladeplantao.com.br/grupos', 'escala_plantao')\n",
    "    all_groups.extend(groups)\n",
    "    print(f\"   ‚úÖ {len(groups)} grupos extra√≠dos\")\n",
    "    \n",
    "    print(f\"\\nüîç GRUPOS_MEDICOS\")\n",
    "    print(f\"   URL: https://gruposmedicos.com.br/\")\n",
    "    groups = crawl_with_requests('https://gruposmedicos.com.br/', 'grupos_medicos')\n",
    "    all_groups.extend(groups)\n",
    "    print(f\"   ‚úÖ {len(groups)} grupos extra√≠dos\")\n",
    "    \n",
    "    # ============================================\n",
    "    # SITES COM JAVASCRIPT (Playwright Async)\n",
    "    # ============================================\n",
    "    \n",
    "    print(f\"\\nüîç QUERO_PLANTAO\")\n",
    "    print(f\"   URL: https://queroplantao.com.br/grupos/\")\n",
    "    try:\n",
    "        groups = await crawl_quero_plantao_async()\n",
    "        all_groups.extend(groups)\n",
    "        print(f\"   ‚úÖ {len(groups)} grupos extra√≠dos\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro: {e}\")\n",
    "    \n",
    "    print(f\"\\nüîç SEARCH_PLANTAO\")\n",
    "    print(f\"   URL: https://web.searchplantao.com.br/grupos\")\n",
    "    try:\n",
    "        groups = await crawl_with_playwright_async(\n",
    "            'https://web.searchplantao.com.br/grupos', \n",
    "            'search_plantao',\n",
    "            wait_time=8000\n",
    "        )\n",
    "        all_groups.extend(groups)\n",
    "        print(f\"   ‚úÖ {len(groups)} grupos extra√≠dos\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro: {e}\")\n",
    "    \n",
    "    print(f\"\\nüîç PLANTOES_BRASIL\")\n",
    "    print(f\"   URL: https://plantoesmedicosbrasil.com.br/\")\n",
    "    try:\n",
    "        groups = await crawl_with_playwright_async(\n",
    "            'https://plantoesmedicosbrasil.com.br/', \n",
    "            'plantoes_brasil',\n",
    "            wait_time=8000\n",
    "        )\n",
    "        all_groups.extend(groups)\n",
    "        print(f\"   ‚úÖ {len(groups)} grupos extra√≠dos\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"   TOTAL BRUTO: {len(all_groups)} grupos\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return all_groups\n",
    "\n",
    "# Executa os crawlers\n",
    "all_groups = asyncio.get_event_loop().run_until_complete(run_all_crawlers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Processar e Deduplica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria DataFrame\n",
    "df = pd.DataFrame(all_groups)\n",
    "\n",
    "if len(df) == 0:\n",
    "    print(\"‚ùå Nenhum grupo encontrado!\")\n",
    "else:\n",
    "    # Remove duplicatas por invite_code\n",
    "    antes = len(df)\n",
    "    df = df.drop_duplicates(subset=['invite_code'], keep='first')\n",
    "    depois = len(df)\n",
    "    \n",
    "    print(f\"\\nüìä DEDUPLICA√á√ÉO:\")\n",
    "    print(f\"   Antes: {antes} grupos\")\n",
    "    print(f\"   Depois: {depois} grupos\")\n",
    "    print(f\"   Removidos: {antes - depois} duplicatas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas\n",
    "if len(df) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"   ESTAT√çSTICAS FINAIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìä POR FONTE:\")\n",
    "    print(df['source'].value_counts().to_string())\n",
    "    \n",
    "    print(f\"\\nüìä POR PLATAFORMA:\")\n",
    "    print(df['platform'].value_counts().to_string())\n",
    "    \n",
    "    print(f\"\\nüìä POR ESTADO (top 15):\")\n",
    "    print(df['state'].value_counts().head(15).to_string())\n",
    "    \n",
    "    print(f\"\\nüìä POR CATEGORIA (top 15):\")\n",
    "    print(df['category'].value_counts().head(15).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filtrar Grupos M√©dicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorias que N√ÉO s√£o de m√©dicos\n",
    "categorias_nao_medicas = [\n",
    "    'enfermagem', 'odontologia', 'fisioterapia', 'psicologia', \n",
    "    'nutricao', 'fonoaudiologia', 'assistencia_social', 'bombeiro',\n",
    "    'socorrista', 'tecnico_enfermagem', 'educacao_fisica', 'farmacia'\n",
    "]\n",
    "\n",
    "# Filtra apenas m√©dicos\n",
    "if len(df) > 0:\n",
    "    df_medicos = df[~df['category'].isin(categorias_nao_medicas)].copy()\n",
    "    \n",
    "    print(f\"\\nü©∫ GRUPOS DE M√âDICOS: {len(df_medicos)} de {len(df)} total\")\n",
    "    print(f\"\\nCategorias inclu√≠das:\")\n",
    "    print(df_medicos['category'].value_counts().to_string())\n",
    "else:\n",
    "    df_medicos = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exportar CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    # Salva todos os grupos\n",
    "    df.to_csv('grupos_whatsapp_saude_COMPLETO.csv', index=False)\n",
    "    print(f\"‚úÖ Salvo: grupos_whatsapp_saude_COMPLETO.csv ({len(df)} grupos)\")\n",
    "    \n",
    "    # Salva apenas m√©dicos\n",
    "    df_medicos.to_csv('grupos_whatsapp_medicos_COMPLETO.csv', index=False)\n",
    "    print(f\"‚úÖ Salvo: grupos_whatsapp_medicos_COMPLETO.csv ({len(df_medicos)} grupos)\")\n",
    "    \n",
    "    # Salva por estado (√∫til para opera√ß√£o)\n",
    "    estados_brasil = ['AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', \n",
    "                      'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN',\n",
    "                      'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO']\n",
    "    df_por_estado = df_medicos[df_medicos['state'].isin(estados_brasil)]\n",
    "    df_por_estado.to_csv('grupos_whatsapp_medicos_POR_ESTADO.csv', index=False)\n",
    "    print(f\"‚úÖ Salvo: grupos_whatsapp_medicos_POR_ESTADO.csv ({len(df_por_estado)} grupos)\")\n",
    "    \n",
    "    # Salva apenas WhatsApp (para Evolution API)\n",
    "    df_whatsapp = df_medicos[df_medicos['platform'] == 'whatsapp']\n",
    "    df_whatsapp.to_csv('grupos_whatsapp_EVOLUTION_API.csv', index=False)\n",
    "    print(f\"‚úÖ Salvo: grupos_whatsapp_EVOLUTION_API.csv ({len(df_whatsapp)} grupos)\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum grupo para salvar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizar Amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_medicos) > 0:\n",
    "    # Mostra amostra por estado\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"   AMOSTRA DE GRUPOS POR ESTADO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for state in ['SP', 'RJ', 'MG', 'BA', 'PR', 'RS', 'DF']:\n",
    "        state_groups = df_medicos[df_medicos['state'] == state]\n",
    "        if len(state_groups) > 0:\n",
    "            print(f\"\\nüìç {state}: {len(state_groups)} grupos\")\n",
    "            for _, row in state_groups.head(3).iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['name'][:50]}\")\n",
    "                print(f\"     C√≥digo: {row['invite_code']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_medicos) > 0:\n",
    "    # Mostra amostra por especialidade\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"   AMOSTRA DE GRUPOS POR ESPECIALIDADE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for cat in ['cardiologia', 'pediatria', 'emergencia', 'ortopedia', 'anestesiologia']:\n",
    "        cat_groups = df_medicos[df_medicos['category'] == cat]\n",
    "        if len(cat_groups) > 0:\n",
    "            print(f\"\\nüè• {cat.upper()}: {len(cat_groups)} grupos\")\n",
    "            for _, row in cat_groups.head(2).iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['name'][:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    df_whatsapp = df_medicos[df_medicos['platform'] == 'whatsapp'] if len(df_medicos) > 0 else pd.DataFrame()\n",
    "    estados_brasil = ['AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', \n",
    "                      'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN',\n",
    "                      'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO']\n",
    "    df_por_estado = df_medicos[df_medicos['state'].isin(estados_brasil)] if len(df_medicos) > 0 else pd.DataFrame()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"   RESUMO FINAL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\"\"\n",
    "üìä GRUPOS EXTRA√çDOS:\n",
    "   Total (todas √°reas):     {len(df)} grupos\n",
    "   Apenas m√©dicos:          {len(df_medicos)} grupos\n",
    "   Por estado:              {len(df_por_estado)} grupos\n",
    "   WhatsApp (Evolution):    {len(df_whatsapp)} grupos\n",
    "\n",
    "üìÅ ARQUIVOS GERADOS:\n",
    "   ‚Ä¢ grupos_whatsapp_saude_COMPLETO.csv\n",
    "   ‚Ä¢ grupos_whatsapp_medicos_COMPLETO.csv\n",
    "   ‚Ä¢ grupos_whatsapp_medicos_POR_ESTADO.csv\n",
    "   ‚Ä¢ grupos_whatsapp_EVOLUTION_API.csv\n",
    "\n",
    "üöÄ PR√ìXIMO PASSO:\n",
    "   Use o arquivo grupos_whatsapp_EVOLUTION_API.csv\n",
    "   para integrar com a Evolution API e fazer a\n",
    "   J√∫lia entrar nos grupos automaticamente.\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum grupo foi extra√≠do. Verifique os erros acima.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. C√≥digo de Integra√ß√£o com Evolution API\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "EVOLUTION_URL = \"https://seu-evolution.com\"\n",
    "API_KEY = \"sua-api-key\"\n",
    "\n",
    "def join_group(instance_name, invite_code):\n",
    "    \"\"\"Entra em um grupo de WhatsApp\"\"\"\n",
    "    url = f\"{EVOLUTION_URL}/group/acceptInvite/{instance_name}\"\n",
    "    \n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json={\"inviteCode\": invite_code},\n",
    "        headers={\"apikey\": API_KEY}\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# L√™ o CSV\n",
    "df = pd.read_csv('grupos_whatsapp_EVOLUTION_API.csv')\n",
    "\n",
    "# Entra em cada grupo (com rate limiting)\n",
    "for i, row in df.iterrows():\n",
    "    result = join_group('julia-01', row['invite_code'])\n",
    "    print(f\"Grupo {row['name']}: {result}\")\n",
    "    \n",
    "    # M√°ximo 5 grupos por dia por n√∫mero\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(\"Limite di√°rio atingido, trocar n√∫mero ou aguardar\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(60)  # 1 minuto entre cada entrada\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
